{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "#%matplotlib inline\n",
    "import time\n",
    "from scipy import io as sio\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# TCN imports \n",
    "import tf_models, utils, metrics, datasets\n",
    "from utils import imshow_\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# import pickle\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** The lables for this has to be numbers ***\n",
    "## Moreover the splits files has to be in a specific format the one that is generated in the \"Preparing the data.ipynb\" has to be corrected. The names must be in this format: 01 02 12 13 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure axes are correct (TxF not FxT for F=feat, T=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper-function for plotting training history\n",
    "def plot_training_history(dir_out,history,name_):\n",
    "    # Make sure folder exists\n",
    "    saveDir = os.path.join(dir_out,'plots')\n",
    "    if not os.path.isdir(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    # Get the classification accuracy and loss-value\n",
    "    # for the training-set.\n",
    "    acc = history.history['acc']\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    # Get it for the validation-set (we only use the test-set).\n",
    "    val_acc = history.history['val_acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the accuracy and loss-values for the training-set.\n",
    "    fig1=plt.figure()\n",
    "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
    "    plt.plot(loss/max(abs(np.array(loss))), 'o', color='b', label='Training Loss')\n",
    "    plt.plot(val_acc, linestyle='-', color='r', label='Validation Acc.')\n",
    "    plt.plot(val_loss/max(abs(np.array(val_loss))), 'o', color='r', label='Validation Loss')\n",
    "    # Plot it for the test-set.\n",
    "#     plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
    "#     plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
    "\n",
    "    # Plot title and legend.\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ensure the plot shows correctly.\n",
    "    plt.show()\n",
    "    fig1.savefig(os.path.join(saveDir,'History_'+name_+'.png'))\n",
    "    fig1.savefig(os.path.join(saveDir,'History_'+name_+'.eps'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- We choose the model properties and the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../\n",
      "Dataset:  UW_IOM\n",
      "model_type: ED-TCN\n",
      "Feature:  PreTrained\n"
     ]
    }
   ],
   "source": [
    "# ---------- Directories & User inputs --------------\n",
    "# Location of data/features folder\n",
    "base_dir = os.path.expanduser(\"../\")\n",
    "print(base_dir)\n",
    "save_predictions = [False, True][1]\n",
    "viz_predictions = [False, True][1]\n",
    "viz_weights = [False, True][0]\n",
    "\n",
    "# Set dataset and action label granularity (if applicable)\n",
    "dataset = [\"UW_IOM\"][0]\n",
    "print('Dataset: ', dataset)\n",
    "granularity = [\"eval\", \"mid\"][0]\n",
    "sensor_type = [\"video\", \"sensors\"][0]\n",
    "\n",
    "# 1 - Set model and parameters\n",
    "model_type = [\"SVM\", \"LSTM\",  \"DilatedTCN\", \"ED-TCN\"][3]\n",
    "print('model_type:', model_type)\n",
    "# causal or acausal? (If acausal use Bidirectional LSTM)\n",
    "causal = [False, True][0]\n",
    "\n",
    "# How many latent states/nodes per layer of network\n",
    "# Only applicable to the TCNs. The ECCV and LSTM  model suses the first element from this list.\n",
    "n_nodes = [64, 96]\n",
    "# 2- Define the number of epochs\n",
    "nb_epoch = 200 # Number of epochs\n",
    "video_rate = 0 # sample rate. We don't have enough data to sample from\n",
    "# 3- Defining the number of convolutional layer\n",
    "conv = {'UW_IOM':20}[dataset]\n",
    "\n",
    "# 4- Which features for the given dataset\n",
    "features = [\"FineTune45\",\"PreTrained\",\"PCNN\"][1] \n",
    "print('Feature: ', features)\n",
    "bg_class = 0 if dataset is not \"JIGSAWS\" else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(dataset+'_'+features+'_'+model_type)\n",
    "if 1:\n",
    "# for conv in [5, 10, 15, 20]:\n",
    "    # Initialize dataset loader & metrics\n",
    "    \n",
    "    data = datasets.Dataset(dataset, base_dir)\n",
    "    print(data)\n",
    "    trial_metrics = metrics.ComputeMetrics(overlap=.1, bg_class=bg_class)\n",
    "#     print(data.splits)\n",
    "    # Load data for each split\n",
    "    for split in data.splits:\n",
    "        t = time.time()\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++' + split + '++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        if sensor_type==\"video\":\n",
    "            feature_type = \"A\" if model_type != \"SVM\" else \"X\"\n",
    "        else:\n",
    "            feature_type = \"S\"\n",
    "\n",
    "        X_train, y_train, X_test, y_test = data.load_split(features, split=split, \n",
    "                                                            sample_rate=video_rate, \n",
    "                                                            feature_type=feature_type)\n",
    "\n",
    "        if trial_metrics.n_classes is None:\n",
    "            trial_metrics.set_classes(data.n_classes)\n",
    "\n",
    "        n_classes = data.n_classes\n",
    "        print('n_classes',n_classes)\n",
    "        train_lengths = [x.shape[0] for x in X_train]\n",
    "        test_lengths = [x.shape[0] for x in X_test]\n",
    "        n_train = len(X_train)\n",
    "        n_test = len(X_test)\n",
    "        print('n_train',n_train ,'n_test',n_test)\n",
    "        n_feat = data.n_features\n",
    "        print(\"# Feat:\", n_feat)\n",
    "\n",
    "        # ------------------ Models ----------------------------\n",
    "        if model_type == \"SVM\":\n",
    "            \n",
    "            svm = LinearSVC()\n",
    "            t1 = time.time()\n",
    "            svm.fit(np.vstack(X_train), np.hstack(y_train))\n",
    "            ttrain = time.time()\n",
    "            print(\"Training time\")\n",
    "            print(ttrain-t1)\n",
    "            print(\"Training plus loading time\")\n",
    "            print(ttrain-t)\n",
    "            AP_train = [svm.decision_function(x) for x in X_train]\n",
    "            t2 = time.time()\n",
    "            P_test = [svm.predict(x) for x in X_test]\n",
    "\n",
    "            # AP_x contains the per-frame probabilities (or class) for each class\n",
    "            AP_test = [svm.decision_function(x) for x in X_test]\n",
    "            ttest = time.time()\n",
    "            print(\"Test time\")\n",
    "            print(ttest - t2)\n",
    "            param_str = \"SVM\"\n",
    "\n",
    "        elif model_type in [\"ED-TCN\", \"DilatedTCN\", \"LSTM\"]:\n",
    "            # Go from y_t = {1...C} to one-hot vector (e.g. y_t = [0, 0, 1, 0])\n",
    "            Y_train = [np_utils.to_categorical(y, n_classes) for y in y_train]\n",
    "            Y_test = [np_utils.to_categorical(y, n_classes) for y in y_test]\n",
    "\n",
    "\n",
    "            # In order process batches simultaneously all data needs to be of the same length\n",
    "            # So make all same length and mask out the ends of each.\n",
    "            n_layers = len(n_nodes)\n",
    "            max_len = max(np.max(train_lengths), np.max(test_lengths))\n",
    "            max_len = int(np.ceil(max_len / (2**n_layers)))*2**n_layers\n",
    "            print(\"Max length:\", max_len)\n",
    "\n",
    "            X_train_m, Y_train_, M_train = utils.mask_data(X_train, Y_train, max_len, mask_value=-1)\n",
    "            X_test_m, Y_test_, M_test = utils.mask_data(X_test, Y_test, max_len, mask_value=-1)\n",
    "\n",
    "            \n",
    "            if model_type == \"ED-TCN\":\n",
    "                model, param_str = tf_models.ED_TCN(n_nodes, conv, n_classes, n_feat, max_len, causal=causal, \n",
    "                                        activation='norm_relu', return_param_str=True)\n",
    "                # plot the model\n",
    "                plot_model(model, to_file=model_type+'_model.png', show_shapes=True)\n",
    "                # model, param_str = tf_models.ED_TCN_atrous(n_nodes, conv, n_classes, n_feat, max_len, \n",
    "                                    # causal=causal, activation='norm_relu', return_param_str=True)                 \n",
    "            elif model_type == \"DilatedTCN\":\n",
    "#                 L = dilation_depth : number of layers per stack\n",
    "#                 B = nb_stacks : number of stacks.\n",
    "                L=3; B = 5\n",
    "                model, param_str = tf_models.Dilated_TCN(n_feat, n_classes, n_nodes[0], L, B, max_len=max_len, \n",
    "                                        causal=causal, return_param_str=True)\n",
    "                plot_model(model, to_file=model_type+'_model.png', show_shapes=True)\n",
    "            elif model_type == \"LSTM\":\n",
    "                model, param_str = tf_models.BidirLSTM(n_nodes[0], n_classes, n_feat, causal=causal, return_param_str=True)\n",
    "                plot_model(model, to_file=model_type+'_model.png', show_shapes=True)\n",
    "            # Run the model\n",
    "            t1 = time.time()\n",
    "            history = model.fit(X_train_m, Y_train_, epochs=nb_epoch, batch_size=8,\n",
    "                        verbose=1,validation_data=[X_test_m,Y_test_] ,sample_weight=M_train[:,:,0]) \n",
    "            ttrain = time.time()\n",
    "            print(\"Training time\")\n",
    "            print(ttrain-t1)\n",
    "            print(\"Training plus loading model time\")\n",
    "            print(ttrain-t)\n",
    "            AP_train = model.predict(X_train_m, verbose=0)\n",
    "            AP_train = utils.unmask(AP_train, M_train)\n",
    "            P_train = [p.argmax(1) for p in AP_train]\n",
    "            \n",
    "            t2 = time.time()\n",
    "            AP_test = model.predict(X_test_m, verbose=0)\n",
    "            AP_test = utils.unmask(AP_test, M_test)\n",
    "\n",
    "            P_test = [p.argmax(1) for p in AP_test]\n",
    "            ttest = time.time()\n",
    "            print(\"Testing time\")\n",
    "            print(ttest-t2)\n",
    "\n",
    "        # --------- Metrics ----------    \n",
    "        trial_metrics.add_predictions(split, P_test, y_test)       \n",
    "        trial_metrics.print_trials()\n",
    "        \n",
    "\n",
    "        # ----- Save predictions -----\n",
    "        if save_predictions:\n",
    "            dir_out = os.path.expanduser(base_dir+\"/predictions/{}/{}/{}/\".format(dataset,features,param_str))\n",
    "            saveDir = os.path.join(dir_out,'plots')\n",
    "            if not os.path.isdir(saveDir):\n",
    "                os.makedirs(saveDir)\n",
    "            # Make sure folder exists\n",
    "            if not os.path.isdir(dir_out):\n",
    "                os.makedirs(dir_out)\n",
    "\n",
    "            out = {\"P\":P_test, \"Y\":y_test, \"S\":AP_test}\n",
    "            sio.savemat( dir_out+\"/{}.mat\".format(split), out)      \n",
    "\n",
    "        # ---- Viz predictions -----\n",
    "        if viz_predictions:\n",
    "            max_classes = data.n_classes - 1\n",
    "            # # Output all truth/prediction pairs\n",
    "            fig = plt.figure(split, figsize=(20,10))\n",
    "            P_test_ = np.array(P_test)#/float(n_classes-1)\n",
    "#             print(P_test); print(y_test)\n",
    "            y_test_ = np.array(y_test)#/float(n_classes-1)\n",
    "            for i in range(len(y_test)):\n",
    "                P_tmp = np.vstack([y_test_[i], P_test_[i]])\n",
    "                plt.subplot(n_test,1,i+1)\n",
    "                imshow_(P_tmp, vmin=0, vmax=1)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "#                 plt.legend()\n",
    "                acc = np.mean(y_test[i]==P_test[i])*100\n",
    "                plt.ylabel(\"Acc: {:.01f}%\".format(acc))\n",
    "#                 plt.title(\"Acc: {:.03}%\".format(100*np.mean(P_test[i]==y_test[i])))\n",
    "            fig.savefig(os.path.join(saveDir,'prediction_'+dataset+'_'+features+'_'+model_type+'_'+split+'.png'))\n",
    "            fig.savefig(os.path.join(saveDir,'prediction_'+dataset+'_'+features+'_'+model_type+'_'+split+'.eps'), format=\"eps\")\n",
    "#             pickle.dump(fig,file('prediction.pickle','w'))\n",
    "        # ---- Viz weights -----\n",
    "        if viz_weights and model_type is \"TCN\":\n",
    "            # Output weights at the first layer\n",
    "            plt.figure(2, figsize=(15,15))\n",
    "            ws = model.get_weights()[0]\n",
    "            for i in range(min(36, len(ws.T))):\n",
    "                plt.subplot(6,6,i+1)\n",
    "                # imshow_(model.get_weights()[0][i][:,:,0]+model.get_weights()[1][i])\n",
    "                imshow_(np.squeeze(ws[:,:,:,i]).T)\n",
    "                # Output weights at the first layer\n",
    "\n",
    "            for l in range(2*n_layers):\n",
    "                plt.figure(l+1, figsize=(15,15))\n",
    "                ws = model.get_weights()[l*2]\n",
    "                for i in range(min(36, len(ws.T))):\n",
    "                    plt.subplot(6,6,i+1)\n",
    "                    # imshow_(model.get_weights()[0][i][:,:,0]+model.get_weights()[1][i])\n",
    "                    imshow_(np.squeeze(ws[:,:,:,i]).T)\n",
    "                    # Output weights at the first layer\n",
    "        #plot_training_history(dir_out,history,features+'_'+model_type+'_'+split)\n",
    "        elapsed = time.time() - t\n",
    "        print('Time elapsed: ', elapsed)\n",
    "    print()\n",
    "    trial_metrics.print_scores()\n",
    "    trial_metrics.print_trials()\n",
    "    print()\n",
    "#     plot_training_history(history,model_type+'_'+split)\n",
    "##### Saving the Model #####\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(dir_out+dataset+'_'+features+'_'+model_type+\".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(dir_out+dataset+'_'+features+'_'+model_type+\".h5\")\n",
    "print(\"Saved model to disk\")\n",
    "fig.savefig(os.path.join(dir_out+'/plots/','prediction_'+dataset+'_'+features+'_'+model_type+'_'+split+'.png'))\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Time elapsed: ', elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model summary:')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
