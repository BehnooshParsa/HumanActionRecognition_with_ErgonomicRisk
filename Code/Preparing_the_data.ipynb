{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from numpy.random import randint\n",
    "import pandas as pd \n",
    "import scipy.io as sc\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 17 classes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_classes = {'none_none_none_none': 0,\n",
    "                   'none_walk_none_none': 1,\n",
    "                   'none_stand_reach_top': 2,\n",
    "                   'box_stand_pick-up_top': 3,\n",
    "                   'box_stand_place_mid': 4,   \n",
    "                   'none_stand_none_none': 5,                \n",
    "                   'rod_stand_pick-up_top': 6,\n",
    "                   'rod_stand_place_mid': 7,\n",
    "                   'box_stand_pick-up_mid': 8,\n",
    "                   'rod_stand_pick-up_mid': 9,\n",
    "                   'none_bend_none_none': 10,\n",
    "                   'rod_bend_pick-up_low': 11,\n",
    "                   'box_bend_place_low': 12,\n",
    "                   'rod_bend_place_low': 13,                   \n",
    "                   'box_stand_place_top': 14,\n",
    "                   'rod_stand_place_top': 15,\n",
    "                   'box_bend_pick-up_low': 16,\n",
    "                   'box_walk_hold_none': 17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='./UW_IOM_Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels format for UW_dataset:\n",
    "# labels=[mid_level, high_level, low_level, ergonomic_risk]\n",
    "\n",
    "low_level=['none','top','mid','low']\n",
    "mid_level_L=['none','walk', 'bend', 'stand'] #, 'twist'\n",
    "mid_level_H=['none','pick-up','place','reach','hold']\n",
    "high_level=['none','box', 'rod']\n",
    "\n",
    "ergonomic_risk = ['none','low-risk','medium-risk','high-risk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFrame(dataDir):    \n",
    "    video_path =os.path.join(dataDir,'Videos')\n",
    "#     print(video_path)\n",
    "    for i, file in enumerate(os.listdir(video_path)):\n",
    "        file_dir = os.path.join(video_path, file)\n",
    "#         print(file_dir)\n",
    "        filename = file[:-4]\n",
    "        print(\"++++++++\",filename,\"++++++++\")\n",
    "        cap = cv2.VideoCapture(file_dir)\n",
    "        # Check if camera opened successfully\n",
    "        if (cap.isOpened()== False): \n",
    "            print(\"Error opening video stream or folder\")\n",
    "        labels = read_labels(filename+'.txt', dataDir)\n",
    "#         \n",
    "        Frames=[]; count=-1; #training_labels=[]\n",
    "        label_count = 0;\n",
    "        # Read until video is completed\n",
    "        while(cap.isOpened()):\n",
    "            # Capture frame-by-frame\n",
    "            count += 1\n",
    "            ret, frame = cap.read()\n",
    "            #if ret == True and Trim_info[i][0]<count<Trim_info[i][1]:\n",
    "            if ret == True and count<len(labels):\n",
    "                # change the folder name for different frame extracion\n",
    "                training_frame_folder_path = os.path.join('./', 'Frames',str(filename)) #or Frames_original\n",
    "                #We make the folder path.\n",
    "                if not os.path.isdir(training_frame_folder_path):\n",
    "                    os.makedirs(training_frame_folder_path)\n",
    "                \n",
    "                training_frame_path = training_frame_folder_path+\"/%d.jpg\" % label_count\n",
    "#                 *******************************************************************\n",
    "                frame=cv2.resize(frame, (224, 224))  # resize the frames\n",
    "                cv2.imwrite(training_frame_path, frame)     # save frame as JPEG file\n",
    "                \n",
    "                label_count += 1\n",
    "                # Press Q on keyboard to  exit\n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            #elif count==Trim_info[i][1]:\n",
    "            else:      \n",
    "                break\n",
    "            \n",
    "        # When everything done, release the video capture object\n",
    "        cap.release()\n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return \n",
    "\n",
    "def read_labels(filename, dataDir):\n",
    "    labels_path =os.path.join(dataDir,'VideoLabels')\n",
    "    with open(os.path.join(labels_path, filename), 'r') as f:\n",
    "        x = f.readlines()\n",
    "    labels=[]\n",
    "    for i, x_i  in enumerate(x):\n",
    "        if i>0:\n",
    "            labels.append([int(x) for x in x_i.split('\\t')[2:-1]][1::2])\n",
    "    return labels\n",
    "\n",
    "def ExtractFrame_in_classs(dataDir):\n",
    "    video_path = os.path.join(dataDir,'Videos')\n",
    "    labels_tag_path = os.path.join(dataDir,'VideoLabels/')\n",
    "    labels_num_path = os.path.join(dataDir,'VideoLabelssNum/')\n",
    " \n",
    "    for i, file in enumerate(os.listdir(video_path)):\n",
    "        \n",
    "        filename = file[:-4]\n",
    "        file_dir = os.path.join(video_path, file)\n",
    "\n",
    "        print(\"++++++++\",filename,\"++++++++\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(file_dir)\n",
    "        # Check if camera opened successfully\n",
    "        if (cap.isOpened()== False): \n",
    "            print(\"Error opening video stream or folder\")\n",
    "\n",
    "        labelfile = open(labels_tag_path+filename+'.txt', 'r')\n",
    "        labels = [line[:-1] for line in labelfile.readlines()]\n",
    "\n",
    "        Frames=[]; count=-1; #training_labels=[]\n",
    "        # Read until video is completed\n",
    "        label_count = 0;\n",
    "        while(cap.isOpened()):\n",
    "            # Capture frame-by-frame\n",
    "            count += 1\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            #if ret == True  and Trim_info[i][0]<count<Trim_info[i][1]:\n",
    "            if ret == True  and count<len(labels):\n",
    "                training_frame_folder_path = os.path.join('./', 'Training_frames_each_video_with_classes',filename,labels[label_count])\n",
    "                #We make the folder path.\n",
    "                if not os.path.isdir(training_frame_folder_path):\n",
    "                    os.makedirs(training_frame_folder_path)\n",
    "                \n",
    "                training_frame_path = training_frame_folder_path+\"/%d_\" % label_count + labels[label_count]+'_'+ str(i+1)+\".jpg\"\n",
    "                frame=cv2.resize(frame, (224, 224))\n",
    "                \n",
    "                \n",
    "                cv2.imwrite(training_frame_path, frame)\n",
    "                    # save frame as JPEG file\n",
    "\n",
    "                label_count += 1\n",
    "                # Press Q on keyboard to  exit\n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            #elif count==Trim_info[i][1]:\n",
    "            else:\n",
    "                break\n",
    "        # When everything done, release the video capture object\n",
    "        cap.release()\n",
    "        # Closes all the frames\n",
    "        cv2.destroyAllWindows()\n",
    "    return \n",
    "\n",
    "def all_frames_in_classes():\n",
    "#     print(img_root_path)\n",
    "    img_root_path = './Training_frames_each_video_with_classes'\n",
    "    saving_path = './Training_frames_All_classes/'\n",
    "    for i, folder in enumerate(os.listdir(img_root_path)):\n",
    "        print(folder)\n",
    "        sub_folder_path = os.path.join(img_root_path,folder)\n",
    "        for j, sub_folder in enumerate(os.listdir(sub_folder_path)):\n",
    "            print(sub_folder)\n",
    "            class_path = os.path.join(saving_path,sub_folder) #this is the destination path\n",
    "            if not os.path.isdir(class_path):\n",
    "                os.makedirs(class_path)\n",
    "            source_path =  os.path.join(sub_folder_path,sub_folder) #this is the source path\n",
    "            for img in os.listdir(source_path):\n",
    "                shutil.move(os.path.join(source_path,img), class_path)\n",
    "\n",
    "    return\n",
    "\n",
    "def make_train_val():\n",
    "#     print(img_root_path)\n",
    "    img_root_path = './Training_frames_All_classes'\n",
    "    print('******** Important: These are not the actual class numbers, they are just counts. Correct class numbers are shown on top of this file. ***************')\n",
    "    for i, file in enumerate(os.listdir(img_root_path)):\n",
    "        \n",
    "        img_path = os.path.join(img_root_path,file)\n",
    "        train_path = os.path.join('./Train_Valid','train',file)\n",
    "#         print(train_path)\n",
    "        if not os.path.isdir(train_path):\n",
    "                    os.makedirs(train_path)\n",
    "        valid_path = os.path.join('./Train_Valid','valid',file)\n",
    "#         print(valid_path)\n",
    "        if not os.path.isdir(valid_path):\n",
    "                    os.makedirs(valid_path)\n",
    "                \n",
    "        print('class',i+1,': ',file)\n",
    "        \n",
    "        data_size = len(os.listdir(img_path))\n",
    "        val_size = int(.2*data_size)\n",
    "        val_ind = randint(0,data_size-1,val_size)\n",
    "        print('data_size:',data_size)\n",
    "        print('train_size:',data_size-val_size)\n",
    "        print('val_size:',val_size)\n",
    "        count=0\n",
    "        while count<data_size:\n",
    "#             print(valid_path+'/'+os.listdir(img_path)[0])\n",
    "            if count in val_ind:\n",
    "                shutil.move(img_path+'/'+os.listdir(img_path)[0], valid_path+'/'+os.listdir(img_path)[0])\n",
    "            else:\n",
    "                shutil.move(img_path+'/'+os.listdir(img_path)[0], train_path+'/'+os.listdir(img_path)[0])\n",
    "##             os.rename(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\") does the same but ...\n",
    "            count+=1\n",
    "        print('count:', count)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Extracts frames of every video in the dataset and stores the frames in a folder with the video name. This is going to be used later for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++ 01 ++++++++\n",
      "++++++++ 02 ++++++++\n",
      "++++++++ 03 ++++++++\n",
      "++++++++ 04 ++++++++\n",
      "++++++++ 05 ++++++++\n",
      "++++++++ 06 ++++++++\n",
      "++++++++ 07 ++++++++\n",
      "++++++++ 08 ++++++++\n",
      "++++++++ 09 ++++++++\n",
      "++++++++ 10 ++++++++\n",
      "++++++++ 11 ++++++++\n",
      "++++++++ 12 ++++++++\n",
      "++++++++ 13 ++++++++\n",
      "++++++++ 14 ++++++++\n",
      "++++++++ 15 ++++++++\n",
      "++++++++ 16 ++++++++\n",
      "++++++++ 17 ++++++++\n",
      "++++++++ 18 ++++++++\n",
      "++++++++ 19 ++++++++\n",
      "++++++++ 20 ++++++++\n"
     ]
    }
   ],
   "source": [
    "ExtractFrame(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Make a folder for every video with subfolders containing the frames corressponding to a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++ 01 ++++++++\n",
      "++++++++ 02 ++++++++\n",
      "++++++++ 03 ++++++++\n",
      "++++++++ 04 ++++++++\n",
      "++++++++ 05 ++++++++\n",
      "++++++++ 06 ++++++++\n",
      "++++++++ 07 ++++++++\n",
      "++++++++ 08 ++++++++\n",
      "++++++++ 09 ++++++++\n",
      "++++++++ 10 ++++++++\n",
      "++++++++ 11 ++++++++\n",
      "++++++++ 12 ++++++++\n",
      "++++++++ 13 ++++++++\n",
      "++++++++ 14 ++++++++\n",
      "++++++++ 15 ++++++++\n",
      "++++++++ 16 ++++++++\n",
      "++++++++ 17 ++++++++\n",
      "++++++++ 18 ++++++++\n",
      "++++++++ 19 ++++++++\n",
      "++++++++ 20 ++++++++\n"
     ]
    }
   ],
   "source": [
    "ExtractFrame_in_classs(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Put together all frames corresponding to a class and move them all in one folder containing all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "02\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "03\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "04\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "05\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "06\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "07\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "08\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "09\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "10\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "11\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "12\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "13\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "14\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "15\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "16\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "17\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "18\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "19\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n",
      "20\n",
      "box_bend_pick-up_low\n",
      "box_bend_place_low\n",
      "box_stand_pick-up_mid\n",
      "box_stand_pick-up_top\n",
      "box_stand_place_mid\n",
      "box_stand_place_top\n",
      "box_walk_hold_none\n",
      "none_bend_none_none\n",
      "none_stand_none_none\n",
      "none_stand_reach_top\n",
      "none_walk_none_none\n",
      "rod_bend_pick-up_low\n",
      "rod_bend_place_low\n",
      "rod_stand_pick-up_mid\n",
      "rod_stand_pick-up_top\n",
      "rod_stand_place_mid\n",
      "rod_stand_place_top\n"
     ]
    }
   ],
   "source": [
    "all_frames_in_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Build training and validation sets for VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Important: These are not the actual class numbers, they are just counts. Correct class numbers are shown on top of this file. ***************\n",
      "class 1 :  box_bend_pick-up_low\n",
      "data_size: 924\n",
      "train_size: 740\n",
      "val_size: 184\n",
      "count: 924\n",
      "class 2 :  box_bend_place_low\n",
      "data_size: 1031\n",
      "train_size: 825\n",
      "val_size: 206\n",
      "count: 1031\n",
      "class 3 :  box_stand_pick-up_mid\n",
      "data_size: 4807\n",
      "train_size: 3846\n",
      "val_size: 961\n",
      "count: 4807\n",
      "class 4 :  box_stand_pick-up_top\n",
      "data_size: 917\n",
      "train_size: 734\n",
      "val_size: 183\n",
      "count: 917\n",
      "class 5 :  box_stand_place_mid\n",
      "data_size: 4357\n",
      "train_size: 3486\n",
      "val_size: 871\n",
      "count: 4357\n",
      "class 6 :  box_stand_place_top\n",
      "data_size: 1311\n",
      "train_size: 1049\n",
      "val_size: 262\n",
      "count: 1311\n",
      "class 7 :  box_walk_hold_none\n",
      "data_size: 382\n",
      "train_size: 306\n",
      "val_size: 76\n",
      "count: 382\n",
      "class 8 :  none_bend_none_none\n",
      "data_size: 2195\n",
      "train_size: 1756\n",
      "val_size: 439\n",
      "count: 2195\n",
      "class 9 :  none_stand_none_none\n",
      "data_size: 2575\n",
      "train_size: 2060\n",
      "val_size: 515\n",
      "count: 2575\n",
      "class 10 :  none_stand_reach_top\n",
      "data_size: 1252\n",
      "train_size: 1002\n",
      "val_size: 250\n",
      "count: 1252\n",
      "class 11 :  none_walk_none_none\n",
      "data_size: 1780\n",
      "train_size: 1424\n",
      "val_size: 356\n",
      "count: 1780\n",
      "class 12 :  rod_bend_pick-up_low\n",
      "data_size: 786\n",
      "train_size: 629\n",
      "val_size: 157\n",
      "count: 786\n",
      "class 13 :  rod_bend_place_low\n",
      "data_size: 953\n",
      "train_size: 763\n",
      "val_size: 190\n",
      "count: 953\n",
      "class 14 :  rod_stand_pick-up_mid\n",
      "data_size: 4023\n",
      "train_size: 3219\n",
      "val_size: 804\n",
      "count: 4023\n",
      "class 15 :  rod_stand_pick-up_top\n",
      "data_size: 743\n",
      "train_size: 595\n",
      "val_size: 148\n",
      "count: 743\n",
      "class 16 :  rod_stand_place_mid\n",
      "data_size: 4434\n",
      "train_size: 3548\n",
      "val_size: 886\n",
      "count: 4434\n",
      "class 17 :  rod_stand_place_top\n",
      "data_size: 1121\n",
      "train_size: 897\n",
      "val_size: 224\n",
      "count: 1121\n"
     ]
    }
   ],
   "source": [
    "make_train_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Generating random splits for TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos:  [ 1  2  3  4  5  6  7  8  9 10]\n",
      "./Splits/Split_1\n",
      "Training set 1:  [9, 7, 4, 1, 3, 8, 10]\n",
      "Validation set 1:  [2 5 6]\n",
      "./Splits/Split_2\n",
      "Training set 2:  [9, 4, 6, 5, 7, 2, 10]\n",
      "Validation set 2:  [1 3 8]\n",
      "./Splits/Split_3\n",
      "Training set 3:  [5, 7, 6, 10, 1, 2, 3]\n",
      "Validation set 3:  [4 8 9]\n",
      "./Splits/Split_4\n",
      "Training set 4:  [5, 10, 2, 3, 7, 9, 6]\n",
      "Validation set 4:  [1 4 8]\n",
      "./Splits/Split_5\n",
      "Training set 5:  [3, 2, 9, 7, 6, 10, 1]\n",
      "Validation set 5:  [4 5 8]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "video_count = 10 # number of videos\n",
    "split_count = 5 # number of splits\n",
    "valid_count = 3 # number of validation videos\n",
    "training_count = video_count-valid_count # number of training videos\n",
    "\n",
    "A = np.arange(1,video_count+1)\n",
    "\n",
    "print('Videos: ',A)\n",
    "\n",
    "for i in range(split_count):\n",
    "    split_path = os.path.join('./Splits/','Split_'+str(i+1)) #this is the destination path\n",
    "    print(split_path)\n",
    "    if not os.path.isdir(split_path):\n",
    "        os.makedirs(split_path)\n",
    "    np.savetxt('./Splits/'+'/all.txt',A)\n",
    "    e = random.sample(range(1, video_count+1), training_count)\n",
    "#     print(np.shape(e))\n",
    "    print('Training set '+str(i+1)+': ',e)\n",
    "    np.savetxt(split_path+'/train.txt',e)\n",
    "    v = A[np.where(np.isin(A,e)==False)[0]]\n",
    "#     print(np.shape(v))\n",
    "    print('Validation set '+str(i+1)+': ',v)\n",
    "    np.savetxt(split_path+'/test.txt',v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
